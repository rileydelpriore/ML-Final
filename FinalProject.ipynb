{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f3461d1-0ef0-445d-86bf-584fa90d50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('basketball.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b589fabb-4973-4319-bc8c-634c5dca400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "828b520b-b14e-44d6-ad18-7dfde87c60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess and scale the dataset\n",
    "def preprocess(df):\n",
    "    # Drop rows with missing values in specified columns\n",
    "    df.loc[:,\"SEED\"]=df[\"SEED\"].fillna(0)\n",
    "    df = df.dropna(subset=[\"POSTSEASON\"])#, \"SEED\"])\n",
    "    \n",
    "    # Map values in the \"POSTSEASON\" column\n",
    "    #Can use FACTORIZE here\n",
    "    postseason_mapping = {\"Champions\": 0, \"2ND\": 1, \"F4\": 2, \"E8\": 3, \"S16\": 4, \"R32\": 5, \"R64\": 6, \"R68\" : 7}\n",
    "    df.loc[:,\"POSTSEASON\"] = df[\"POSTSEASON\"].replace(postseason_mapping)\n",
    "    # df.loc[:,\"POSTSEASON\"],_ = pd.factorize(df[\"POSTSEASON\"])\n",
    "    \n",
    "    # Convert object type columns to numerical labels, excluding \"POSTSEASON\"\n",
    "    for column in df.select_dtypes(include=[\"object\"]).columns:\n",
    "        if column != \"POSTSEASON\":\n",
    "            df.loc[:,column], _ = pd.factorize(df[column])\n",
    "                \n",
    "    # Convert object data type to float as pytorch needs float/int\n",
    "    conv_map=dict()\n",
    "    for col in df.loc[:, df.dtypes =='O'].columns:\n",
    "        conv_map[col]=float\n",
    "    df = df.astype(conv_map)\n",
    "   \n",
    "    sorted_df = df.sort_values(by=\"POSTSEASON\")\n",
    "    return sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34859ec5-fe56-4b03-bf3a-afbe263f88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POSTSEASON\"] = df[\"POSTSEASON\"].replace(postseason_mapping)\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POSTSEASON\"] = df[\"POSTSEASON\"].replace(postseason_mapping)\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[\"POSTSEASON\"] = df[\"POSTSEASON\"].replace(postseason_mapping)\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[column], _ = pd.factorize(df[column])\n",
      "/var/folders/_f/985z51xd7t7d6n72bm4tzz700000gn/T/ipykernel_20125/1635016105.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[columns_to_scale] = scaler.fit_transform(df[columns_to_scale])\n"
     ]
    }
   ],
   "source": [
    "#create test, val and training dataframes\n",
    "train_val = (df[df[\"YEAR\"]<2022])\n",
    "train,val=train_test_split(train_val,test_size=0.2)\n",
    "# train = (df[df[\"YEAR\"]==2019])\n",
    "# val = (df[df[\"YEAR\"] == 2021])\n",
    "test = (df[df[\"YEAR\"] >= 2022])\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)\n",
    "val = preprocess(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173da65b-ef2b-4157-96cf-3995109cfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop features which have low correlation to POSTSEASON column\n",
    "features_to_drop=[\"POSTSEASON\", \"TEAM\"]\n",
    "for feature in train.columns[train.corr()['POSTSEASON'].abs()<0.10]:\n",
    "    features_to_drop.append(feature)\n",
    "print(\"Dropping Features: \",features_to_drop)\n",
    "    \n",
    "feature_names = train.drop(features_to_drop, axis=1).columns.tolist()\n",
    "num_vars=len(feature_names)\n",
    "num_classes=train[\"POSTSEASON\"].unique().shape[0]\n",
    "\n",
    "#Scale the data based on training (columns_to_scale == feature_names)\n",
    "columns_to_scale = [col for col in train.columns if col not in features_to_drop]\n",
    "scaler = StandardScaler()\n",
    "#scaler=MinMaxScaler()\n",
    "scaler.fit(train[columns_to_scale])\n",
    "\n",
    "\n",
    "#Transform the selected columns of train/val/test\n",
    "for df_tran in [train,val,test]:\n",
    "    df_tran.loc[:,columns_to_scale]=scaler.transform(df_tran[columns_to_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b51ab02-4347-4f9e-a4d3-23c92757198f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.36536354e-16 -1.66533454e-16 -8.32667268e-17  1.38777878e-16\n",
      " -1.38777878e-17  1.66533454e-16 -9.71445147e-17 -6.93889390e-17\n",
      "  2.42861287e-17  1.07552856e-16  4.85722573e-17 -5.20417043e-18\n",
      " -1.38777878e-16  1.38777878e-17 -1.07552856e-16  9.71445147e-17\n",
      "  1.38777878e-17 -3.46944695e-17 -1.66533454e-16  6.93889390e-18\n",
      " -9.71445147e-17]\n",
      "(0.27956349206349207, 0.27734375, 0.2773612454129461, 0.5294117647058824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rileydelpriore/anaconda3/envs/ml/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#utility function to get dataframes/tensors\n",
    "def make_data(df:pd.DataFrame,feature_names,get_tensor:bool=True):\n",
    "    X_train = df[feature_names]\n",
    "    Y_train = df[\"POSTSEASON\"].astype(np.int64)\n",
    "    if get_tensor:\n",
    "        X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        # Adjust labels to start from 0\n",
    "        Y_train = torch.tensor(Y_train.values, dtype=torch.int64)  # Use long tensor for classification labels\n",
    "    return X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc84448-c5d3-489d-b1c1-3f2c82b30fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run logistic regression\n",
    "def fit_predict(train_fname: pd.DataFrame, test_fname: pd.DataFrame,feature_names:list) -> np.array:\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "    X_train,Y_train=make_data(train,feature_names,False)\n",
    "    X_test,_=make_data(test,feature_names,False)\n",
    "    \n",
    "    classifier.fit(X_train, Y_train)\n",
    "    Y_pred = classifier.predict(X_test)\n",
    "    coefs = [0]*num_vars\n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_vars):\n",
    "            coefs[j] += classifier.coef_[i][j]\n",
    "    #print(coefs)\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    weights = np.mean(classifier.coef_, axis = 0)\n",
    "    weights=weights/np.abs(weights).sum()\n",
    "    feature_importance = {feature_names[i]: abs(weights[i]) for i in range(len(feature_names))}\n",
    "\n",
    "    sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    top_n = 10\n",
    "    print(f\"Top {top_n} features from Logistic Regressopn:\")\n",
    "    for feature, importance in sorted_features[:top_n]:\n",
    "        print(f\"{feature}: {importance:.4f}\")\n",
    "    return Y_pred.astype(np.int64),sorted_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3344edf4-6ff8-4321-aaa6-d556f047f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(data: pd.DataFrame, Y_pred: np.array) -> list[float]:\n",
    "    Y_true = data[\"POSTSEASON\"].astype(np.int64).values\n",
    "    precision = metrics.precision_score(Y_true, Y_pred, average = \"macro\",zero_division=np.nan)\n",
    "    recall = metrics.recall_score(Y_true, Y_pred, average = \"macro\")\n",
    "    f1 = metrics.f1_score(Y_true, Y_pred, average = \"macro\")\n",
    "    accuracy=metrics.accuracy_score(Y_true, Y_pred)\n",
    "    #accuracy = (Y_pred == Y_true).mean() * 100\n",
    "    return accuracy,precision, recall, f1\n",
    "\n",
    "Y_pred,sorted_features_LR = fit_predict(train, test,feature_names)\n",
    "print(\"=\"*80)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "acc,prec,rec,f1=score(test,Y_pred)\n",
    "print(f\"Accuracy={acc:.4f} Precision={prec:.4f} Recall={rec:.4f} F1={f1:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d10c83-cccd-4633-8ff9-468c87595769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9145177   1.73949932  2.29117439 ... -2.77589493  1.77912944\n",
      "  -1.67116778]\n",
      " [-1.02841368  1.73949932  1.34262122 ... -0.34304084  0.98941238\n",
      "  -1.24233982]\n",
      " [-0.80062172  2.23859617  1.57975951 ...  0.05050908  1.70208388\n",
      "  -1.4567538 ]\n",
      " ...\n",
      " [ 2.27456966 -1.75417864 -1.74017659 ... -0.73659077 -2.45837676\n",
      "   1.54504191]\n",
      " [ 2.38846564 -0.25688809 -0.79162342 ...  1.33849065 -1.72644387\n",
      "   1.54504191]\n",
      " [-0.45893379 -0.75598494 -0.55448513 ...  0.62294534 -0.01218\n",
      "   0.47297201]]\n"
     ]
    }
   ],
   "source": [
    "class FNNModel(torch.nn.Module):\n",
    "    def __init__(self, input_size,hidden_size):\n",
    "        num_nodes=hidden_size\n",
    "        layer=[num_nodes,num_nodes//2]\n",
    "        super(FNNModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, layer[0])\n",
    "        self.fc2 = torch.nn.Linear(layer[0], layer[1])\n",
    "        self.fc3 = torch.nn.Linear(layer[1], num_classes)  # Adjust output units to num_classes\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9370cc-4a28-4acf-95b8-764d2c3375f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "       # torch.nn.init.xavier_uniform_(m.weight)\n",
    "        torch.nn.init.kaiming_uniform_(m.weight, a=0, mode=\"fan_in\", nonlinearity=\"relu\") \n",
    "        # torch.nn.init.normal_(m.weight, mean=0, std=1.0e-4) \n",
    "        m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15ec4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X,Y,model):\n",
    "    # Forward pass to get predictions\n",
    "    with torch.no_grad():\n",
    "        output = model(X)\n",
    "    \n",
    "    # Get the predicted labels\n",
    "    _, Y_pred = torch.max(output, -1)\n",
    "    return (Y==Y_pred).float().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070b7db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training,validation and testing data\n",
    "best_overall_acc=-torch.inf\n",
    "best_overall_loss=torch.inf\n",
    "best_feature_set=None\n",
    "best_metrics=None\n",
    "debug=False\n",
    "#hyper parameter loop\n",
    "for num_features in [num_vars]:\n",
    "   \n",
    "   \n",
    "    fnn_features=[]  \n",
    "    for feature in feature_names:\n",
    "        fnn_features.append(feature)\n",
    "    \n",
    "    X_tensor_train,Y_tensor_train=make_data(train,fnn_features)\n",
    "    X_tensor_val,Y_tensor_val=make_data(val,fnn_features)\n",
    "    X_tensor_test,Y_tensor_test=make_data(test,fnn_features)\n",
    "   \n",
    "    for hidden_size in [32,64,128,256,512,1024]:\n",
    "    \n",
    "        model = FNNModel(X_tensor_train.shape[1],hidden_size)  # Adjust input_size according to your data\n",
    "        model.apply(init_weights)\n",
    "        # Define loss function and optimizer\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.0001,weight_decay=1.0e-2)\n",
    "        # optimizer = torch.optim.SGD(model.parameters(), momentum=0.9,nesterov=True,lr=0.0001,weight_decay=1.0e-2)\n",
    "        # Training loop\n",
    "        epochs = 500\n",
    "        best_loss=torch.inf\n",
    "        best_acc=-torch.inf\n",
    "        for epoch in range(epochs):\n",
    "           \n",
    "           \n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            batch_size=16\n",
    "            num_samples=X_tensor_train.shape[0]\n",
    "            num_batches=num_samples//batch_size\n",
    "            for i in range(num_batches):\n",
    "                batch_start=i*batch_size\n",
    "                batch_end= min(batch_start+batch_size,num_samples)\n",
    "                # Forward pass\n",
    "                output = model(X_tensor_train[batch_start:batch_end])\n",
    "                # Calculate the loss\n",
    "                loss = criterion(output, Y_tensor_train[batch_start:batch_end])\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            #calculate loss on val data\n",
    "            with torch.no_grad():\n",
    "                output = model(X_tensor_val)\n",
    "                # Calculate the loss\n",
    "                loss_val = criterion(output, Y_tensor_val)\n",
    "                val_acc=compute_accuracy(X_tensor_val,Y_tensor_val, model)\n",
    "                #Prevent overfit\n",
    "                # if val_acc<best_acc: #ACCURACY AS CRIT.\n",
    "                if loss_val>best_loss:    #LOSS AS CRIT.\n",
    "                    if debug:\n",
    "                        print(f'Epoch [{epoch+1:3d}/{epochs:3d}], Training Loss: {loss.item():.4f} Val Loss:{loss_val.item():.4f}')\n",
    "                        print(\"Stopping since val loss increasing\")\n",
    "                    #load the best params (https://discuss.pytorch.org/t/how-to-save-the-best-model/84608)\n",
    "                    model.load_state_dict(torch.load('best-model.pt'))\n",
    "                    break\n",
    "                else:\n",
    "                    best_acc=val_acc\n",
    "                    best_loss=loss_val\n",
    "                    #save the best params\n",
    "                    torch.save(model.state_dict(), 'best-model.pt')\n",
    "                    \n",
    "                \n",
    "            \n",
    "            # Print training loss every few epochs\n",
    "            if (epoch+1) % 10 == 0 and debug:\n",
    "                print(f'Epoch [{epoch+1:3d}/{epochs:3d}], Training Loss: {loss.item():.4f} Val Loss:{loss_val.item():.4f}')\n",
    "        \n",
    "        #use loss on val data to track best model\n",
    "        #val_acc=compute_accuracy(X_tensor_val,Y_tensor_val,model)\n",
    "        # if (best_acc>best_overall_acc):\n",
    "        if (best_loss<best_overall_loss):\n",
    "            best_overall_acc=best_acc\n",
    "            best_overall_loss=best_loss\n",
    "            # Extract avergae weights from the first layer of the model\n",
    "            weights = model.fc1.weight.detach().numpy()\n",
    "            weights=weights.mean(0).squeeze()\n",
    "            weights=weights/np.abs(weights).sum()\n",
    "            \n",
    "            \n",
    "            # Calculate feature importance\n",
    "            feature_importance = {fnn_features[i]: abs(weights[i]) for i in range(len(fnn_features))}\n",
    "            \n",
    "            # Sort features by importance\n",
    "            sorted_features_NN = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "           \n",
    "            # Print top N features\n",
    "            if debug:\n",
    "                print(f\"Top {num_features} features from FFN:\")\n",
    "                for feature, importance in sorted_features_NN:\n",
    "                    print(f\"{feature}:{importance:8.4f}\")\n",
    "            \n",
    "            # Put the model in evaluation mode\n",
    "            model.eval()\n",
    "            \n",
    "            # Forward pass to get predictions\n",
    "            with torch.no_grad():\n",
    "                output = model(X_tensor_test)\n",
    "            \n",
    "            # Get the predicted labels\n",
    "            _, predicted_labels = torch.max(output, -1)\n",
    "            \n",
    "            \n",
    "            # Calculate accuracy using score utility\n",
    "            Y_pred=predicted_labels.numpy()\n",
    "            #Y_pred[:]+=1 # add 1 since index is 0 based and class 1 based\n",
    "            print(f\"Found a better model with  val loss {best_loss:.4f} and accuracy {best_acc:.4f}\")\n",
    "            print(\"Feature Set: \",fnn_features)\n",
    "            print(\"Hidden Size: \",hidden_size)\n",
    "            acc,prec,rec,f1=score(test,Y_pred)\n",
    "            if debug:\n",
    "                if debug:\n",
    "                    print(\"-\"*80)\n",
    "                    \n",
    "                    print(f\"Test Accuracy={acc:.4f} Precision={prec:.4f} Recall={rec:.4f} F1={f1:.4f}, Loss={best_loss:.4f}\")\n",
    "                    print(\"-\"*80)\n",
    "\n",
    "        \n",
    "        \n",
    "            \n",
    "            best_feature_set=sorted_features_NN\n",
    "            best_metrics=(acc,prec,rec,f1,best_loss,num_features,hidden_size)\n",
    "    \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b38c1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "(acc,prec,rec,f1,best_loss,num_features,hidden_size)=best_metrics\n",
    "print(f\"FFN Best Fit with {num_features} features {hidden_size} hidden size\")\n",
    "print(f\"Validation Accuracy {best_overall_acc:.4f}\")\n",
    "print(f\"Test Accuracy={acc:.4f} Precision={prec:.4f} Recall={rec:.4f} F1={f1:.4f}, Loss={best_loss:.4f}\")\n",
    "print(\"Best feature set by importance: \")\n",
    "for feature, importance in best_feature_set:\n",
    "    print(f\"{feature}:{importance:8.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "def graphs(sorted_features,method): \n",
    "    features = [feat[0] for feat in sorted_features]\n",
    "    importance = [feat[1] for feat in sorted_features]\n",
    "    # Plot the feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(features, importance, color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance for '+method)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top\n",
    "    plt.show()\n",
    "\n",
    "graphs(sorted_features_LR,\"LR\")\n",
    "graphs(sorted_features_NN,\"NN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
